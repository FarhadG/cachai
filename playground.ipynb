{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "if 'src' not in os.listdir('.'):\n",
    "    os.chdir('../')\n",
    "\n",
    "import src.utils.models as M\n",
    "import src.utils.constants as C\n",
    "import src.utils.evaluation as E\n",
    "from src.utils.runner import Runner\n",
    "from config.debugger_config import debugger_experiments\n",
    "from config.aggregate_config import aggregate_experiments\n",
    "from config.increment_config import increment_experiments\n",
    "from scripts.run_experiment import run_experiment\n",
    "from src.utils.dataframe import build_metadata_df, build_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def tune_hyperparameters(config: M.ExperimentConfig):\n",
    "    def objective(trial):\n",
    "        hyperparams = {\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'invscaling', 'adaptive']),\n",
    "            'eta0': trial.suggest_float('eta0', 0.001, 0.1, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 0.0001, 0.01, log=True),\n",
    "            'penalty': trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet']),\n",
    "            'tol': trial.suggest_categorical('tol', [1e-3, 1e-4]),\n",
    "            'max_iter': trial.suggest_int('max_iter', 1, 10000, log=True),\n",
    "            'warm_start': trial.suggest_categorical('warm_start', [True, False]),\n",
    "        }\n",
    "        result = run_experiment(config, hyperparams)\n",
    "        return result[C.RMSE]\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_hyperparams = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f'Best hyperparameters: {best_hyperparams} with loss {best_score}')\n",
    "\n",
    "    return best_hyperparams, study.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(parallel=False)\n",
    "# runner = Runner(parallel=True)\n",
    "\n",
    "# runner.run(run_experiment, [\n",
    "# *debugger_experiments,\n",
    "# *aggregate_experiments,\n",
    "# *increment_experiments,\n",
    "# ])\n",
    "\n",
    "config = debugger_experiments[0]\n",
    "# run_experiment(config)\n",
    "best_hyperparams, tuning_results = tune_hyperparameters(config)\n",
    "# Optionally: Run a final experiment with the best hyperparameters\n",
    "print(\"Running final experiment with best hyperparameters...\")\n",
    "final_result = run_experiment(config, best_hyperparams)\n",
    "print(f\"Final experiment result: {final_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = build_metadata_df('results/**/config.json')\n",
    "# metadata_df = metadata_df[metadata_df[C.EXPERIMENT_NAME].str.contains('Debugger|Aggregate|Increment')]\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df = build_df(metadata_df, C.EXPERIMENT_LOGGER)\n",
    "experiments_df.sort_values(by=C.OBSERVATION_TIME, inplace=True)\n",
    "experiments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    experiments_df,\n",
    "    x=C.ITERATION,\n",
    "    y=C.Y_PRED,\n",
    "    color=C.EXPERIMENT_NAME,\n",
    "    title='Predictions over time',\n",
    "    hover_data=experiments_df.columns\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    experiments_df,\n",
    "    x=C.ITERATION,\n",
    "    y=C.MAE,\n",
    "    color=C.EXPERIMENT_NAME,\n",
    "    title='MAE over time',\n",
    "    hover_data=experiments_df.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    experiments_df,\n",
    "    x=C.OBSERVATION_TIME,\n",
    "    y=C.MAE,\n",
    "    color=C.EXPERIMENT_NAME,\n",
    "    title='MAE on observation time',\n",
    "    hover_data=experiments_df.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(\n",
    "    experiments_df,\n",
    "    y=C.MAE,\n",
    "    title=C.MAE,\n",
    "    labels={'y': C.MAE},\n",
    "    color=C.EXPERIMENT_NAME,\n",
    "    hover_data=experiments_df.columns\n",
    ").update_traces(boxmean=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    experiments_df,\n",
    "    x=C.ITERATION,\n",
    "    y=C.HITS,\n",
    "    color=C.EXPERIMENT_NAME,\n",
    "    title='Hits over time',\n",
    "    hover_data=experiments_df.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_eval_df = E.evaluate_experiment_loss_group(experiments_df)\n",
    "experiments_eval_df.sort_values(by=C.MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(\n",
    "    experiments_eval_df.sort_values(by=C.MAE).melt(\n",
    "        id_vars=C.EXPERIMENT_NAME,\n",
    "        value_vars=[C.RMSE, C.MAE]\n",
    "    ),\n",
    "    x=C.EXPERIMENT_NAME,\n",
    "    y='value',\n",
    "    color='variable',\n",
    "    barmode='group',\n",
    "    labels={'value': 'Loss Metrics', 'variable': 'Metric'},\n",
    "    title='Loss Metrics (RMSE and MAE)',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(\n",
    "    experiments_eval_df.sort_values(by=C.MBE),\n",
    "    x=C.EXPERIMENT_NAME,\n",
    "    y=C.MBE,\n",
    "    title='Loss Metrics (MBE)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachais_df = build_df(metadata_df, C.CACHAI_LOGGER)\n",
    "cachais_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachais_eval_df = E.evaluate_experiment_cache_metrics_group(cachais_df)\n",
    "cachais_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(\n",
    "    cachais_eval_df.sort_values(by=C.CACHE_HIT_PRECISION).melt(\n",
    "        id_vars=C.EXPERIMENT_NAME,\n",
    "        value_vars=[\n",
    "            C.CACHE_SERVE_RATE, C.CACHE_HIT_PRECISION, C.CACHE_HIT_ACCURACY, C.CACHE_STALE_RATE, C.CACHE_MISS_RATE\n",
    "        ]\n",
    "    ),\n",
    "    x=C.EXPERIMENT_NAME,\n",
    "    y='value',\n",
    "    color='variable',\n",
    "    barmode='group',\n",
    "    labels={'value': 'Rate', 'variable': 'Metric'},\n",
    "    title='Cache Metrics'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(\n",
    "    cachais_eval_df.sort_values(by=C.CACHE_HIT_TOTAL),\n",
    "    x=C.EXPERIMENT_NAME,\n",
    "    y=C.CACHE_HIT_TOTAL,\n",
    "    title='Cache Hit Total'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cachai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
